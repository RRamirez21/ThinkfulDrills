{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge:Choosing_A_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DNgxgOgKkUj0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Challenge: what model can answer this question?\n",
        "You now have a fairly substantial starting toolbox of supervised learning methods that you can use to tackle a host of exciting problems. To make sure all of these ideas are organized in your mind, please go through the list of problems below. For each, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor."
      ]
    },
    {
      "metadata": {
        "id": "8v2N9SppkY5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
        "\n",
        "Because this is a continuous variable, we should use a regression (linear, knn, random forest, svm, etc.). If there are a high amount of features, and we might want to discard them, we may employ Lasso Regression as well.\n"
      ]
    },
    {
      "metadata": {
        "id": "P1uO0DQ2kZF9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. You have more features (columns) than rows in your dataset.\n",
        "\n",
        "Given the large number of features, we should use Ridge Regression to regularize the problem. Again, we can employ Lasso Regression to reduce the number of predictors or attempt Gradient Boosting, if feature importance is helpful. We might want to use PCA to reduce the dimensionality of the data. \n"
      ]
    },
    {
      "metadata": {
        "id": "uE9tgJ4pkZV5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
        "\n",
        "Since we are looking for the most important characteristic, first we would use normalization to even the range of features. Then we could use Gradient Boosting or Random Forest because both have feature ranking capability. The size of the coefficients of a regularized regression model is also indicative of feature importance.  \n"
      ]
    },
    {
      "metadata": {
        "id": "urEoH8eFkZgh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4. Implement a filter to “highlight” emails that might be important to the recipient\n",
        "\n",
        "Because this is categorical, we can use Naive Bayes to \"highlight\" emails in a similar capacity to how it is regularly used for spam filtering. We can also employ Support Vector Classification since it performs well on high-dimensional datasets like text classifiers.\n"
      ]
    },
    {
      "metadata": {
        "id": "aleh8-7xkZqG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "5. You have 1000+ features.\n",
        "\n",
        "Given the large amount of features, we should use Lasso to force small parameter estimates to be equal to zero, effectively dropping them from the model. We may also use Gradient Boosting to identify our most relevant features."
      ]
    },
    {
      "metadata": {
        "id": "F1y62cDnkZyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "6. Predict whether someone who adds items to their cart on a website will purchase the items.\n",
        "\n",
        "This is a classification so we should use a Random Forest or Decision Tree or any other binary classification method. However, if we would like to ouput a probability, we can also use Naive Bayes since there is conditional probablity involved.\n"
      ]
    },
    {
      "metadata": {
        "id": "T9G4FWGGkZ7K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "7. Your dataset dimensions are 982400 x 500\n",
        "\n",
        "Given the large number of variables, we should use Lasso Regression to reduce the features or Gradient Boosting to identify the most relevant features.\n"
      ]
    },
    {
      "metadata": {
        "id": "YjDP4506kaD_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "8. Identify faces in an image.\n",
        "\n",
        "If this a face segmentation problem, a semantic segmentation algorithm would be appropriate (U-net, SegNet, FCN, etc.). If this were an object detection problem, an algorithm like YOLO would work. If this were a classification problem, identifying the faces of particular people, this could be tackled by any multiclass classification algorithm.   "
      ]
    },
    {
      "metadata": {
        "id": "xWVifvwGkaMf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "9. Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
        "\n",
        "This is also a classification so we could use a Decision Tree or Random Forest or a SVM or Gradient Boosting or Multiclass Logistic Regression or a Multilayer Perceptron. We could also achieve this with a Naive Bayes. "
      ]
    }
  ]
}
